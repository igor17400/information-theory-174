\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\bibstyle{apalike}
\citation{mackay_book,willey_info_theory}
\babel@aux{english}{}
\citation{GoodBengCour16}
\citation{feller1,GoodBengCour16}
\citation{blitzstein2024}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction to Information Theory}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Basic Concepts in Probability Theory}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.0.1}Sample spaces}{3}{subsubsection.2.0.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the sample space as a pebble world, with two events A and B spotlighted\relax }}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sample_space}{{1}{3}{Illustration of the sample space as a pebble world, with two events A and B spotlighted\relax }{figure.caption.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Sets Symbols Dictionary\relax }}{4}{table.caption.3}\protected@file@percent }
\newlabel{tab:notation}{{1}{4}{Sets Symbols Dictionary\relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Naive Definition of Probability}{4}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}How to Count}{4}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Tree Diagram.\relax }}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:tree_diagram}{{2}{4}{Tree Diagram.\relax }{figure.caption.4}{}}
\citation{feller1}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of sampling with replacement.\relax }}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:sampling_replacement}{{3}{5}{Illustration of sampling with replacement.\relax }{figure.caption.5}{}}
\citation{feller1}
\citation{venkatesh2024}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}\csqQQ {34}Leibniz mistake\csqQQ {34}}{6}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Adjusting for Overcounting}{6}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Conditional Probability And Stochastic Independence}{6}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Conditional Probability}{6}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Bayes' Rule For Events}{9}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Intuitive Explanation of Bayes' Rule}{9}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Illustration of how to understand Bayes' formula.\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:bayes_schematic}{{4}{9}{Illustration of how to understand Bayes' formula.\relax }{figure.caption.6}{}}
\citation{mackay_book}
\citation{mackay_youtube_playlist}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Side information}{10}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Introduction to Information Theory}{10}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}How can we achieve perfect communication over an imperfect, noisy communication channel?}{11}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Binary Symmetric Channel (BSC)\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{fig:bsc}{{5}{11}{Binary Symmetric Channel (BSC)\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}What do we mean by \emph  {channel}?}{11}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Process of the Encoding of a Message\relax }}{12}{figure.caption.8}\protected@file@percent }
\newlabel{fig:encoder_schematic}{{6}{12}{Process of the Encoding of a Message\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Adding Redundancy}{12}{subsubsection.4.1.2}\protected@file@percent }
\citation{mackay_book}
\citation{mackay_book}
\citation{mackay_book}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Likelihood ratios and decoded sequences for a binary symmetric channel \cite  {mackay_book}.\relax }}{13}{table.caption.9}\protected@file@percent }
\newlabel{tab:likelihood_decoding}{{2}{13}{Likelihood ratios and decoded sequences for a binary symmetric channel \cite {mackay_book}.\relax }{table.caption.9}{}}
\citation{mackay_book,feller1}
\@writefile{toc}{\contentsline {section}{\numberline {5}Markov Chains and Information Source}{14}{section.5}\protected@file@percent }
\citation{stationary_distributions}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.0.1}Markov Information Source}{15}{subsubsection.5.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.0.2}Stationary Distributions}{15}{subsubsection.5.0.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Two State Markov Chain\relax }}{16}{figure.caption.10}\protected@file@percent }
\newlabel{fig:two_state_markov}{{7}{16}{Two State Markov Chain\relax }{figure.caption.10}{}}
\citation{Sachdeva_MarkovChains}
\citation{willey_info_theory}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Particle movement with markov chains\relax }}{17}{figure.caption.11}\protected@file@percent }
\newlabel{fig:markov_example_1}{{8}{17}{Particle movement with markov chains\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Entropy, Relative Entropy, and Mutual Information}{17}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Likelihood of the user being in each of the four states (Home, Work, Park, Bar) at any given step.\relax }}{18}{figure.caption.12}\protected@file@percent }
\newlabel{fig:markov_example_2}{{9}{18}{Likelihood of the user being in each of the four states (Home, Work, Park, Bar) at any given step.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Entropy}{18}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Joint Entropy and Conditional Entropy}{18}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Relative Entropy and Mutual Information}{19}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Chain Rules For Entropy, Relative Entropy, and Mutual Information}{19}{subsection.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Relationship between entropy and mutual information.\relax }}{20}{figure.caption.13}\protected@file@percent }
\newlabel{fig:venn}{{10}{20}{Relationship between entropy and mutual information.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Jensen's Inequality and its Consequences}{20}{subsection.6.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Illustration of a striclty convex, convex, and non convex functions\relax }}{20}{figure.caption.14}\protected@file@percent }
\newlabel{fig:convex_function}{{11}{20}{Illustration of a striclty convex, convex, and non convex functions\relax }{figure.caption.14}{}}
\newlabel{theo:deriv_convex}{{9}{20}{Jensen's Inequality and its Consequences}{theorem.9}{}}
\newlabel{theo:jensen}{{10}{20}{Jensen's Inequality}{theorem.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Illustration of the second derivative per type of function - Theorem \ref {theo:deriv_convex}.\relax }}{21}{figure.caption.15}\protected@file@percent }
\newlabel{fig:convex_derivative}{{12}{21}{Illustration of the second derivative per type of function - Theorem \ref {theo:deriv_convex}.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Illustration of Jensen's Inequality - Theorem \ref {theo:jensen}.\relax }}{22}{figure.caption.16}\protected@file@percent }
\newlabel{fig:jessen_ine}{{13}{22}{Illustration of Jensen's Inequality - Theorem \ref {theo:jensen}.\relax }{figure.caption.16}{}}
\newlabel{theo:ind_bound_entropy}{{14}{22}{Independence bound on entropy}{theorem.14}{}}
\newlabel{lemma:fundamental_inequality}{{1}{22}{Fundamental Inequality}{lemma.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Data Processing Inequality}{22}{subsection.6.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Example of the independence bound on entropy - Theorem \ref {theo:ind_bound_entropy}.\relax }}{23}{figure.caption.17}\protected@file@percent }
\newlabel{fig:independence_bound_entropy}{{14}{23}{Example of the independence bound on entropy - Theorem \ref {theo:ind_bound_entropy}.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Example of the fundamental inequality - Lemma \ref {lemma:fundamental_inequality}.\relax }}{23}{figure.caption.18}\protected@file@percent }
\newlabel{fig:fundamental_inequality}{{15}{23}{Example of the fundamental inequality - Lemma \ref {lemma:fundamental_inequality}.\relax }{figure.caption.18}{}}
\newlabel{theo:data_proc_ineq}{{17}{23}{Data Processing Inequality}{theorem.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}Sufficient Statistics}{23}{subsection.6.7}\protected@file@percent }
\citation{willey_info_theory}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Example of the data processing inequality - Theorem \ref {theo:data_proc_ineq}.\relax }}{24}{figure.caption.19}\protected@file@percent }
\newlabel{fig:data_process_inequality}{{16}{24}{Example of the data processing inequality - Theorem \ref {theo:data_proc_ineq}.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8}Fano's Inequality}{24}{subsection.6.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Data Compression}{24}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Example of Codes}{24}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Kraft Inequality}{24}{subsection.7.2}\protected@file@percent }
\bibdata{ResearchPaperBib}
\bibcite{blitzstein2024}{{1}{2024}{{Blitzstein}}{{}}}
\bibcite{willey_info_theory}{{2}{2006}{{Cover and Thomas}}{{}}}
\bibcite{feller1}{{3}{1968}{{Feller}}{{}}}
\bibcite{GoodBengCour16}{{4}{2016}{{Goodfellow et~al.}}{{}}}
\bibcite{stationary_distributions}{{5}{2017}{{GSIS-Tohuku}}{{}}}
\bibcite{mackay_youtube_playlist}{{6}{nd}{{MacKay}}{{}}}
\bibcite{mackay_book}{{7}{2002}{{MacKay}}{{}}}
\bibcite{Sachdeva_MarkovChains}{{8}{2022}{{Sachdeva}}{{}}}
\bibcite{venkatesh2024}{{9}{2024}{{Venkatesh}}{{}}}
\gdef \@abspage@last{25}
